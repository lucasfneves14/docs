---
title: 'Quickstart'
description: 'Build your first RAG pipeline in minutes'
---

This quickstart gets you up and running with GraphorLM as quickly as possible. It introduces you to the platform's interface and core RAG pipeline components without diving into technical details or complex configurations.

In this tutorial, you will:
- Load your data into GraphorLM
- Configure a basic RAG pipeline with intelligent chunking
- Test your pipeline

## Step one: Set up GraphorLM Project

GraphorLM Cloud provides a fully-managed environment where you can build, test, and deploy RAG pipelines without worrying about infrastructure or complex setup. The free trial gives you full access to all platform features.

1. Sign up for a [GraphorLM Cloud account](https://app.graphorlm.com/register)
2. Login to your account [Login](https://app.graphorlm.com/login)
2. Once logged in, you'll see the GraphorLM dashboard
3. Select **New project** and name it as you want

<img
  className="block border rounded-2xl border-gray-950\/10 ring-2 ring-transparent"
  src="/images/project-list-demo.png"
  alt="Project list demo"
/>

Once your project is created, you'll be taken to the sources dashboard where you can begin building your RAG pipeline.

## Step two: Load your data

GraphorLM allows you to easily upload your own documents to use in your RAG pipeline.

1. In your project, navigate to **Sources** in the left sidebar
2. Click **Add Sources** to open the data upload interface
3. Choose your preferred import method:
   - **Local files**: Upload files from your computer (PDF, MD, PNG, JPG, DOCX, TXT, CSV and others are supported)
   - **URL**: Import content directly from a web address
4. Select or drag and drop your files into the upload area
5. Click **Finish** to begin the ingestion process

<img
  className="block border rounded-2xl border-gray-950\/10 ring-2 ring-transparent"
  src="/images/sources-demo.png"
  alt="Sources demo"
/>

During processing, GraphorLM will automatically:
- Extract text using advanced OCR for images and scanned documents
- Identify document structure and metadata
- Prepare your content for chunking in the pipeline

You can monitor the ingestion progress on the Sources dashboard.

After upload, you can click on any file to:
- Customize the parsing method and configure element classification to improve document structure recognition
- Preview the extracted content before pipeline processing

<img
  className="block border rounded-2xl border-gray-950\/10 ring-2 ring-transparent"
  src="/images/parsing-demo.png"
  alt="Parsing demo"
/>

## Step three: Create a RAG pipeline

Now let's create a RAG flow to process your documents and build an intelligent question-answering system.

1. In the left sidebar, click **Flows** to navigate to the Flows dashboard
2. Click **New flow** to start creating your pipeline
3. Enter a descriptive name for your flow (e.g., "DocumentQA")
4. Click **Create** to open the visual flow builder

You'll see the visual flow builder interface with components that can be connected to form your RAG pipeline:

- **Dataset**: Select the documents you uploaded in the previous step
- **Chunking**: Configure how your documents are segmented for optimal retrieval
- **Retrieval**: Define how relevant information is searched and ranked
- **Testset**: Create evaluation scenarios with ground-truth answers
- **Questions**: Define sample queries to test your pipeline
- **LLM**: Connect to language models for generating responses
- **Evaluation**: Measure the quality and accuracy of your pipeline
- **Response**: View the final output of your RAG system

<img
  className="block border rounded-2xl border-gray-950\/10 ring-2 ring-transparent"
  src="/images/blank-flow-demo.png"
  alt="Blank flow demo"
/>

The flow builder uses a drag-and-drop interface where you can connect these components to create your custom RAG pipeline.

## Step four: Configure pipeline components

Now let's build your RAG pipeline by adding and connecting the essential components:

1. Add the **Dataset** component
   - Drag the Dataset component from the sidebar onto the canvas
   - Double-click on the component to open its configuration panel
   - Select the sources you imported earlier from the files menu
   - Close the configuration panel
   - This component provides your documents to the pipeline

2. Add the **Chunking** component
   - Drag the Chunking component onto the canvas
   - Connect the output of the Dataset component to the input of the Chunking component
   - Double-click to configure with these recommended settings:
     - **Embedding / Indexer**: text-embedding-3-small (provides semantic understanding)
     - **Elements to remove**: None (or select elements to exclude)
     - **Splitter**: Smart chunking (maintains semantic coherence)
     - **Chunk Size**: 5000 (adjust based on your document complexity)
   - Close the configuration panel
   - This component divides your documents into smaller pieces for efficient retrieval

3. Add the **Retrieval** component
   - Drag the Retrieval component onto the canvas
   - Connect the output of the Chunking component to the input of the Retrieval
   - Double-click to configure:
     - **Search Type**: Similarity (recommended for semantic relevance)
     - **Top K**: 5 (number of chunks to retrieve)
     - **Score Threshold**: 0.5 (minimum relevance score)
   - Close the configuration panel
   - This component searches your chunked documents for relevant information

4. Add the **Response** component
   - Drag the Response component onto the canvas
   - Connect the output of the Retrieval component to the input of the Response
   - This component formats and displays the final output of your pipeline

<img
  className="block border rounded-2xl border-gray-950\/10 ring-2 ring-transparent"
  src="/images/simple-flow-demo.png"
  alt="Simple flow demo"
/>

After configuring each component, you can test individual nodes by:
- Double-clicking the component to open its configuration panel
- Clicking **Update results** to see the component's output based on current settings
- Examining the processed data to verify it meets your expectations

**Important Note**: To fully simulate a flow with retrieval nodes, you must add either:
- A **Question** node: For testing individual queries
- A **Testset** node: For batch testing multiple questions

These input nodes provide the questions that drive the retrieval process in your RAG pipeline.

## Step five: Test your RAG pipeline

Let's test the pipeline with some sample queries:

1. Navigate to the **Chat** tab in the left sidebar
2. Select your "QuickstartPipeline" from the dropdown
3. Type a question related to the technical documentation, such as: "What are the system requirements?"
4. Click **Send** to see the response

The response combines information retrieved from the documents with natural language generation from the LLM. Notice how the system:
- Retrieves relevant chunks from the documentation
- Highlights the source of information
- Provides a coherent response

## Next steps

Congratulations! You've built and tested your first RAG pipeline with GraphorLM. Here's what you can explore next:

- Learn how to [upload your own documents](/guides/data-ingestion)
- Explore [advanced chunking strategies](/guides/chunking) for better retrieval
- Understand [evaluation metrics](/guides/evaluation) to improve your RAG pipeline
- Connect your pipeline to [external applications](/guides/integrate-workflow) via API
